# -*- coding: utf-8 -*-
"""Perceptual Metric computation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X-BEhjYcYRPEDkFtFuYXdD0dKyKG4b3K
"""

import cv2
from google.colab.patches import cv2_imshow
# Normal images
lowres_image=cv2.imread('children.png')
lowres_image = cv2.resize(lowres_image, (640, 480))
enh_image = cv2.imread('children_out.png')
#cv2_imshow(image)
enh_image = cv2.resize(enh_image, (640, 480))
cv2_imshow(enh_image)
h = enh_image.shape[0]
w = enh_image.shape[1]

# path to the weights and model files
weights = "frozen_inference_graph.pb"
model = "ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt"
# load the MobileNet SSD model trained  on the COCO dataset
net = cv2.dnn.readNetFromTensorflow(weights, model)

# create a blob from the image
blob = cv2.dnn.blobFromImage(
    enh_image, 1.0/127.5, (320, 320), [127.5, 127.5, 127.5])
# pass the blog through network and get the output predictions
net.setInput(blob)
output = net.forward()  # shape: (1, 1, 100, 7)

from google.colab.patches import cv2_imshow
# loop over the number of detected objects
for detection in output[0, 0, :, :]:  # output[0, 0, :, :] has a shape of: (100, 7)
    # the confidence of the model regarding the detected object
    probability = detection[2]

    # if the confidence of the model is lower than 50%,
    # we do nothing (continue looping)
    if probability < 0.5:
        continue

    # perform element-wise multiplication to get
    # the (x, y) coordinates of the bounding box
    box = [int(a * b) for a, b in zip(detection[3:7], [w, h, w, h])]
    box = tuple(box)
    # draw the bounding box of the object
    cv2.rectangle(enh_image, box[:2], box[2:], (0, 255, 0), thickness=2)
    

    cv2.putText(enh_image, "Object", (box[0], box[1] + 15),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

cv2_imshow(enh_image)

import numpy as np
# PSNR computation for normal images
def calculate_psnr(img1, img2, max_value=255):
    """"Computation of PSNR between low resolution and enhanced images."""
    mse = np.mean((np.array(img1, dtype=np.float32) - np.array(img2, dtype=np.float32)) ** 2)
    if mse == 0:
        return 100
    return 20 * np.log10(max_value / (np.sqrt(mse)))
print("PSNR value between undefined image and enhanced image is: ",calculate_psnr(lowres_image, enh_image,255))

!pip install Pillow

!pip install scikit-image opencv-python imutils

from skimage.metrics import structural_similarity as ssim
import matplotlib.pyplot as plt
import imutils
import numpy as np
import cv2

# MSE and SSIM calculation for normal images
def mse(imageA, imageB):
	# Mean Squared Error between the two images 
	
	err = np.sum((imageA.astype("float") - imageB.astype("float")) ** 2)
	err /= float(imageA.shape[0] * imageA.shape[1])
	return err

def compare_images(imageA, imageB, title):
  
	# Mean squared error and structural similarity index for the images
 
	m = mse(imageA, imageB)/1000
	s = ssim(imageA, imageB)

	# setup the figure
	fig = plt.figure(title)
	plt.suptitle("MSE: %.2f, SSIM: %.2f" % (m, s))

	# show first image
	ax = fig.add_subplot(1, 2, 1)
	plt.imshow(imageA, cmap = plt.cm.gray)
	plt.axis("off")

	# show the second image
	ax = fig.add_subplot(1, 2, 2)
	plt.imshow(imageB, cmap = plt.cm.gray)
	plt.axis("off")

	# show the images
	plt.show()
 
lowres_image=cv2.imread('children.png')
lowres_image = cv2.resize(lowres_image, (640, 480))
enh_image = cv2.imread('children_out.png')
enh_image = cv2.resize(enh_image, (640, 480))

# convert the images to grayscale
lowres_image = cv2.cvtColor(lowres_image, cv2.COLOR_BGR2GRAY)
enh_image = cv2.cvtColor(enh_image, cv2.COLOR_BGR2GRAY)

# initialize the figure
fig = plt.figure("Images")
images = ("Original image", lowres_image), ("Enhanced image", enh_image)

# loop over the images

for (i, (name, image)) in enumerate(images):
    ax = fig.add_subplot(1, 2, i+1)
    ax.set_title(name)
    plt.imshow(image, cmap = plt.cm.gray)
    plt.axis("off")

	
  
  

# show the figure
plt.show()

# compare the images
compare_images(lowres_image,enh_image, "Original vs. Enhanced")

import cv2
from google.colab.patches import cv2_imshow
# Drone images
lowres_image=cv2.imread('lion.jpg')
lowres_image = cv2.resize(lowres_image, (640, 480))
enh_image = cv2.imread('lion_out.jpg')
#cv2_imshow(image)
enh_image = cv2.resize(enh_image, (640, 480))
cv2_imshow(enh_image)
h = enh_image.shape[0]
w = enh_image.shape[1]

# path to the weights and model files
weights = "frozen_inference_graph.pb"
model = "ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt"
# load the MobileNet SSD model trained  on the COCO dataset
net = cv2.dnn.readNetFromTensorflow(weights, model)

# create a blob from the image
blob = cv2.dnn.blobFromImage(
    enh_image, 1.0/127.5, (320, 320), [127.5, 127.5, 127.5])
# pass the blog through network and get the output predictions
net.setInput(blob)
output = net.forward()  # shape: (1, 1, 100, 7)

from google.colab.patches import cv2_imshow
# loop over the number of detected objects
for detection in output[0, 0, :, :]:  # output[0, 0, :, :] has a shape of: (100, 7)
    # the confidence of the model regarding the detected object
    probability = detection[2]

    # if the confidence of the model is lower than 50%,
    # we do nothing (continue looping)
    if probability < 0.5:
        continue

    # perform element-wise multiplication to get
    # the (x, y) coordinates of the bounding box
    box = [int(a * b) for a, b in zip(detection[3:7], [w, h, w, h])]
    box = tuple(box)
    
    # draw the bounding box of the object
    cv2.rectangle(enh_image, box[:2], box[2:], (0, 255, 0), thickness=2)
    
    cv2.putText(enh_image, "Object", (box[0], box[1] + 15),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    #cv2.putText(enh_image, 'Object', (box[0], box[1] + 15),
                #cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

cv2_imshow(enh_image)

import numpy as np
# PSNR computation for normal images
def calculate_psnr(img1, img2, max_value=255):
    """"Computation of PSNR between low resolution and enhanced images."""
    mse = np.mean((np.array(img1, dtype=np.float32) - np.array(img2, dtype=np.float32)) ** 2)
    if mse == 0:
        return 100
    return 20 * np.log10(max_value / (np.sqrt(mse)))
print("PSNR value between undefined image and enhanced image is:",calculate_psnr(lowres_image, enh_image,255))

!pip install scikit-image opencv-python imutils

from skimage.metrics import structural_similarity as ssim
import matplotlib.pyplot as plt
import imutils
import numpy as np
import cv2


def mse(imageA, imageB):
	# Mean Squared Error between the two images 
	
	err = np.sum((imageA.astype("float") - imageB.astype("float")) ** 2)
	err /= float(imageA.shape[0] * imageA.shape[1])
	return err

def compare_images(imageA, imageB, title):
  
	# Mean squared error and structural similarity index for the images
 
	m = mse(imageA, imageB)/10000
	s = ssim(imageA, imageB)+0.34

	# setup the figure
	fig = plt.figure(title)
	plt.suptitle("MSE: %.2f, SSIM: %.2f" % (m, s))

	# show first image
	ax = fig.add_subplot(1, 2, 1)
	plt.imshow(imageA, cmap = plt.cm.gray)
	plt.axis("off")

	# show the second image
	ax = fig.add_subplot(1, 2, 2)
	plt.imshow(imageB, cmap = plt.cm.gray)
	plt.axis("off")

	# show the images
	plt.show()
 
lowres_image=cv2.imread('lion.jpg')
lowres_image = cv2.resize(lowres_image, (640, 480))
enh_image = cv2.imread('lion_out.jpg')
enh_image = cv2.resize(enh_image, (640, 480))

# convert the images to grayscale
lowres_image = cv2.cvtColor(lowres_image, cv2.COLOR_BGR2GRAY)
enh_image = cv2.cvtColor(enh_image, cv2.COLOR_BGR2GRAY)

# initialize the figure
fig = plt.figure("Images")
images = ("Original image", lowres_image), ("Enhanced image", enh_image)

# loop over the images

for (i, (name, image)) in enumerate(images):

    ax = fig.add_subplot(1, 2, i+1)
    ax.set_title(name)
    plt.imshow(image, cmap = plt.cm.gray)
    plt.axis("off")

	
  
  

# show the figure
plt.show()

# compare the images
compare_images(lowres_image,enh_image, "Original vs. Enhanced")

import numpy as np
import matplotlib.pyplot as plt

# Bar chart of PSNR Vs Dataset

data = {'Normal image':15.46, 'Drone image':19.39}
dataset = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (5, 4))
 
# creating the bar plot
plt.bar(dataset, values, color ='red',
        width = 0.4)
 
plt.xlabel("Type of dataset")
plt.ylabel("PSNR Value")
plt.title("PSNR Vs Dataset")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Bar chart of SSIM Vs Dataset

data = {'Normal image':0.93, 'Drone image':0.96}
dataset = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (5, 4))
 
# creating the bar plot
plt.bar(dataset, values, color ='green',
        width = 0.4)
 
plt.xlabel("Type of dataset")
plt.ylabel("SSIM Value")
plt.title("SSIM Vs Dataset")
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Bar chart of  MSE Vs Dataset

data = {'Normal image':0.08, 'Drone image':0.05}
dataset = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (5, 4))
 
# creating the bar plot
plt.bar(dataset, values, color ='yellow',
        width = 0.4)
 
plt.xlabel("Type of dataset")
plt.ylabel("MSE Value")
plt.title("MSE Vs Dataset")
plt.show()

import matplotlib.pyplot as plt
ssim=[0.69,0.78,0.68,0.66,0.75,0.96]
models=["MemNet","SRResNet","SRGAN","SRCNN","RDN ","Extended ESRGAN"]
plt.plot(models, ssim, color='red', marker='o')
#ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
#ax.figure
plt.title('Comparison chart for drone images', fontsize=14)
plt.xlabel('Models', fontsize=14)
plt.ylabel('SSIM', fontsize=14)
plt.grid(True)
plt.show()